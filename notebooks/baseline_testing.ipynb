{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3542add1",
   "metadata": {},
   "source": [
    "# Baseline LM Sampling Analysis (CPU)\n",
    "\n",
    "This notebook implements a **simple, end-to-end baseline pipeline** in one place:\n",
    "- load TinyStories,\n",
    "- train/use a BPE tokenizer,\n",
    "- build 256-token chunks,\n",
    "- train a small decoder-only Transformer,\n",
    "- run sanity checks,\n",
    "- save artifacts.\n",
    "\n",
    "It keeps the core tokenizer design from `tokenizer.py`, but allows small in-notebook adjustments without editing the original file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63401790",
   "metadata": {},
   "source": [
    "## 1) Set Up Notebook Environment and MCP Jupyter Workflow\n",
    "\n",
    "This notebook is designed to be executed cell-by-cell (including via MCP notebook tools).\n",
    "\n",
    "`RUN_MODE` controls scale:\n",
    "- `quick` → development on ~100k stories\n",
    "- `full` → baseline submission run on full train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ba1b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "MPS available: True\n",
      "Run mode: full\n",
      "Workspace: /Users/m3/Documents/uni/s26/genai/lm_sampling_analysis\n"
     ]
    }
   ],
   "source": [
    "# Optional: uncomment if running in a fresh environment\n",
    "# %pip install -q datasets torch tqdm tokenizers\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "RUN_MODE = \"full\"  # \"quick\" or \"full\"\n",
    "WORKSPACE = Path.cwd()\n",
    "ARTIFACTS_DIR = WORKSPACE / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"Run mode: {RUN_MODE}\")\n",
    "print(f\"Workspace: {WORKSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496604b",
   "metadata": {},
   "source": [
    "## 2) Load the Tokenizer Backend and Inspect Its Internal Structure\n",
    "\n",
    "We use Hugging Face `tokenizers` BPE and inspect the internal components:\n",
    "- model (BPE),\n",
    "- pre-tokenizer (ByteLevel),\n",
    "- decoder,\n",
    "- learned vocabulary/special token mappings after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer backend: Tokenizer\n",
      "Model: BPE\n",
      "Pre-tokenizer: ByteLevel\n",
      "Decoder: ByteLevel\n"
     ]
    }
   ],
   "source": [
    "base_tok = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
    "base_tok.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "base_tok.decoder = ByteLevelDecoder()\n",
    "\n",
    "print(\"Tokenizer backend:\", type(base_tok).__name__)\n",
    "print(\"Model:\", type(base_tok.model).__name__)\n",
    "print(\"Pre-tokenizer:\", type(base_tok.pre_tokenizer).__name__)\n",
    "print(\"Decoder:\", type(base_tok.decoder).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cea2de",
   "metadata": {},
   "source": [
    "## 3) Create an In-Notebook Tokenizer Variant (No Source File Changes)\n",
    "\n",
    "This variant preserves the original BPE principle (byte-level base + merge sequence), while adding notebook-only helpers:\n",
    "- save/load,\n",
    "- special-token registration,\n",
    "- convenience encode/decode wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356cdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFNotebookBPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "        self.special_tokens = {}\n",
    "\n",
    "    def train(self, corpus_sents, vocab_size=10_000, special_tokens=(\"<PAD>\", \"<EOS>\", \"<UNK>\")):\n",
    "        trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=list(special_tokens), show_progress=True)\n",
    "        self.tokenizer.train_from_iterator(corpus_sents, trainer=trainer)\n",
    "\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.special_tokens = {token: int(vocab[token]) for token in special_tokens if token in vocab}\n",
    "\n",
    "        if \"<EOS>\" in self.special_tokens:\n",
    "            self.tokenizer.post_processor = TemplateProcessing(\n",
    "                single=\"$A <EOS>\",\n",
    "                pair=\"$A <EOS> $B:1 <EOS>:1\",\n",
    "                special_tokens=[(\"<EOS>\", self.special_tokens[\"<EOS>\"])],\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return set(self.tokenizer.get_vocab().values())\n",
    "\n",
    "    def encode(self, text: str, add_eos: bool = False):\n",
    "        if add_eos:\n",
    "            return self.tokenizer.encode(text).ids\n",
    "        ids = self.tokenizer.encode(text).ids\n",
    "        if \"<EOS>\" in self.special_tokens and ids and ids[-1] == self.special_tokens[\"<EOS>\"]:\n",
    "            return ids[:-1]\n",
    "        return ids\n",
    "\n",
    "    def encode_batch_with_eos(self, texts):\n",
    "        return [enc.ids for enc in self.tokenizer.encode_batch(texts)]\n",
    "\n",
    "    def decode_ids(self, ids):\n",
    "        return self.tokenizer.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "    def to_state(self):\n",
    "        return {\n",
    "            \"tokenizer_json\": self.tokenizer.to_str(),\n",
    "            \"special_tokens\": self.special_tokens,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        tok = cls()\n",
    "        tok.tokenizer = Tokenizer.from_str(state[\"tokenizer_json\"])\n",
    "        tok.special_tokens = {k: int(v) for k, v in state.get(\"special_tokens\", {}).items()}\n",
    "        return tok\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        path = Path(path)\n",
    "        path.write_text(self.tokenizer.to_str(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ce89a",
   "metadata": {},
   "source": [
    "## 4) Apply Minimal Tokenization Adjustments While Preserving Core Principles\n",
    "\n",
    "We train BPE using the same core algorithm, then add minimal localized behavior:\n",
    "- register `<PAD>` and `<EOS>` after merge training,\n",
    "- append `<EOS>` at sequence boundaries in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06409994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories train split...\n",
      "Stories used: 2,119,719\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer trained in 2.26 min\n",
      "Vocabulary size: 10000\n",
      "Special tokens: {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2}\n"
     ]
    }
   ],
   "source": [
    "MAX_STORIES_QUICK = 100_000\n",
    "TOKENIZER_VOCAB_SIZE = 10_000\n",
    "CONTEXT_LEN = 256\n",
    "\n",
    "print(\"Loading TinyStories train split...\")\n",
    "train_ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "\n",
    "if RUN_MODE == \"quick\":\n",
    "    train_ds = train_ds.select(range(min(MAX_STORIES_QUICK, len(train_ds))))\n",
    "\n",
    "train_texts = [row[\"text\"] for row in train_ds]\n",
    "print(f\"Stories used: {len(train_texts):,}\")\n",
    "\n",
    "tok = HFNotebookBPETokenizer()\n",
    "start = time.time()\n",
    "tok.train(train_texts, vocab_size=TOKENIZER_VOCAB_SIZE, special_tokens=(\"<PAD>\", \"<EOS>\", \"<UNK>\"))\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Tokenizer trained in {(end-start)/60:.2f} min\")\n",
    "print(\"Vocabulary size:\", len(tok.vocabulary))\n",
    "print(\"Special tokens:\", tok.special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e3fa8",
   "metadata": {},
   "source": [
    "## 5) Implement Invariant Checks for Vocabulary, Special Tokens, and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d925e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded length: 15\n",
      "Decoded preview: Once upon a time, a little cat played with a red ball.<EOS>\n",
      "Invariant checks passed ✅\n"
     ]
    }
   ],
   "source": [
    "assert len(tok.vocabulary) <= TOKENIZER_VOCAB_SIZE, \"Vocab unexpectedly exceeds target\"\n",
    "assert \"<PAD>\" in tok.special_tokens and \"<EOS>\" in tok.special_tokens and \"<UNK>\" in tok.special_tokens\n",
    "\n",
    "sample = \"Once upon a time, a little cat played with a red ball.\"\n",
    "enc = tok.encode(sample, add_eos=True)\n",
    "dec = tok.decode_ids(enc)\n",
    "\n",
    "print(\"Encoded length:\", len(enc))\n",
    "print(\"Decoded preview:\", dec[:120])\n",
    "print(\"Invariant checks passed ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c1169",
   "metadata": {},
   "source": [
    "## 6) Integrate the Tokenizer Variant into the Implementation Pipeline\n",
    "\n",
    "The pipeline below uses the notebook tokenizer variant for both training data preparation and later generation/inference usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e69334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9052e3b63d4c1f9e342329da9eaa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding stories:   0%|          | 0/2071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 464,966,457\n",
      "First 20 token ids: [317, 253, 14, 156, 294, 343, 397, 261, 492, 156, 3629, 213, 206, 655, 16, 210, 600, 201, 179, 2859]\n"
     ]
    }
   ],
   "source": [
    "def encode_story_with_eos(text: str, tokenizer):\n",
    "    return tokenizer.encode(text, add_eos=True)\n",
    "\n",
    "def flatten_encoded_stories(texts, tokenizer, batch_size=1024):\n",
    "    all_ids = []\n",
    "    for start_idx in tqdm(range(0, len(texts), batch_size), desc=\"Encoding stories\"):\n",
    "        batch = texts[start_idx:start_idx + batch_size]\n",
    "        batch_ids = tokenizer.encode_batch_with_eos(batch)\n",
    "        for ids in batch_ids:\n",
    "            all_ids.extend(ids)\n",
    "    return all_ids\n",
    "\n",
    "all_token_ids = flatten_encoded_stories(train_texts, tok)\n",
    "print(\"Total tokens:\", f\"{len(all_token_ids):,}\")\n",
    "print(\"First 20 token ids:\", all_token_ids[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5cedd",
   "metadata": {},
   "source": [
    "## 7) Encode Dataset Samples and Build Batching Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c8c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks shape: (1816275, 256)\n",
      "Train chunks: (1725461, 256) Val chunks: (90814, 256)\n",
      "Batch x/y/mask shapes: torch.Size([8, 255]) torch.Size([8, 255]) torch.Size([8, 255])\n"
     ]
    }
   ],
   "source": [
    "def build_chunks(token_ids, context_len=256):\n",
    "    usable = (len(token_ids) // context_len) * context_len\n",
    "    token_ids = token_ids[:usable]\n",
    "    chunks = np.array(token_ids, dtype=np.int64).reshape(-1, context_len)\n",
    "    return chunks\n",
    "\n",
    "chunks = build_chunks(all_token_ids, context_len=CONTEXT_LEN)\n",
    "print(\"Chunks shape:\", chunks.shape)\n",
    "\n",
    "split_idx = int(0.95 * len(chunks))\n",
    "train_chunks = chunks[:split_idx]\n",
    "val_chunks = chunks[split_idx:]\n",
    "print(\"Train chunks:\", train_chunks.shape, \"Val chunks:\", val_chunks.shape)\n",
    "\n",
    "PAD_ID = tok.special_tokens[\"<PAD>\"]\n",
    "EOS_ID = tok.special_tokens[\"<EOS>\"]\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, array_2d):\n",
    "        self.data = torch.tensor(array_2d, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx, :-1]\n",
    "        y = self.data[idx, 1:]\n",
    "        attention_mask = torch.ones_like(x)\n",
    "        return x, y, attention_mask\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = 32\n",
    "\n",
    "train_loader = DataLoader(LMDataset(train_chunks), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(LMDataset(val_chunks), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "x0, y0, m0 = next(iter(train_loader))\n",
    "print(\"Batch x/y/mask shapes:\", x0.shape, y0.shape, m0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75ede32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 8.34M\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).bool()\n",
    "        scores = scores.masked_fill(~mask, torch.finfo(scores.dtype).min)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.drop(attn)\n",
    "\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_len=256, d_model=256, n_heads=4, n_layers=4, ff_dim=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(context_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "VOCAB_SIZE_MODEL = max(tok.vocabulary) + 1\n",
    "model = TinyGPT(\n",
    "    vocab_size=VOCAB_SIZE_MODEL,\n",
    "    context_len=CONTEXT_LEN - 1,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model params: {n_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948726dd",
   "metadata": {},
   "source": [
    "## 8) Run End-to-End Smoke Tests in Notebook Cells\n",
    "\n",
    "This section validates tokenization → batching → model forward/backward on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a766f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke logits shape: (8, 255, 10000)\n",
      "Sanity initial loss: 9.3805\n",
      "Sanity final loss:   7.3497\n",
      "Sanity check pass: True\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(model, loader, max_batches=50):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y, _) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "def train_steps(model, loader, optimizer, max_steps=200, grad_accum_steps=1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    step = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        (loss / grad_accum_steps).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        step += 1\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Forward-pass smoke test\n",
    "x_smoke, y_smoke, _ = next(iter(train_loader))\n",
    "x_smoke = x_smoke.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits_smoke = model(x_smoke)\n",
    "print(\"Smoke logits shape:\", tuple(logits_smoke.shape))\n",
    "\n",
    "# Overfit sanity check on 512 chunks\n",
    "sanity_chunks = train_chunks[:512]\n",
    "sanity_loader = DataLoader(LMDataset(sanity_chunks), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "sanity_model = TinyGPT(\n",
    "    vocab_size=VOCAB_SIZE_MODEL,\n",
    "    context_len=CONTEXT_LEN - 1,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "sanity_opt = torch.optim.AdamW(sanity_model.parameters(), lr=3e-4)\n",
    "sanity_losses = train_steps(sanity_model, sanity_loader, sanity_opt, max_steps=250, grad_accum_steps=GRAD_ACCUM_STEPS)\n",
    "\n",
    "print(f\"Sanity initial loss: {sanity_losses[0]:.4f}\")\n",
    "print(f\"Sanity final loss:   {sanity_losses[-1]:.4f}\")\n",
    "print(\"Sanity check pass:\", sanity_losses[-1] < sanity_losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbb1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main train initial loss: 9.3417\n",
      "Main train final loss:   2.8716\n",
      "Validation loss:         2.7249\n",
      "Validation perplexity:   15.26\n"
     ]
    }
   ],
   "source": [
    "# Short baseline train run (extend max_steps for stronger baseline)\n",
    "main_opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "train_losses = train_steps(model, train_loader, main_opt, max_steps=10000, grad_accum_steps=GRAD_ACCUM_STEPS)\n",
    "val_loss = evaluate_loss(model, val_loader, max_batches=40)\n",
    "val_ppl = math.exp(val_loss) if np.isfinite(val_loss) else float(\"inf\")\n",
    "\n",
    "print(f\"Main train initial loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Main train final loss:   {train_losses[-1]:.4f}\")\n",
    "print(f\"Validation loss:         {val_loss:.4f}\")\n",
    "print(f\"Validation perplexity:   {val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1047c4b",
   "metadata": {},
   "source": [
    "## 9) Compare Original vs Modified Tokenizer Outputs on Target Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532c68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEXT: Once upon a time, a little cat was happy.\n",
      "len no_eos: 11 | len with_eos: 12\n",
      "no_eos first ids: [328, 344, 156, 293, 14, 156, 294, 598, 179, 301, 16]\n",
      "with_eos first ids: [328, 344, 156, 293, 14, 156, 294, 598, 179, 301, 16, 1]\n",
      "decode with_eos preview: Once upon a time, a little cat was happy.<EOS>\n",
      "================================================================================\n",
      "TEXT: The sun is bright, and the kids can play outside!\n",
      "len no_eos: 12 | len with_eos: 13\n",
      "no_eos first ids: [270, 630, 304, 1012, 14, 162, 160, 1195, 368, 256, 575, 3]\n",
      "with_eos first ids: [270, 630, 304, 1012, 14, 162, 160, 1195, 368, 256, 575, 3, 1]\n",
      "decode with_eos preview: The sun is bright, and the kids can play outside!<EOS>\n",
      "================================================================================\n",
      "TEXT: I have 2 apples, you have 3 apples.\n",
      "len no_eos: 10 | len with_eos: 11\n",
      "no_eos first ids: [43, 359, 7348, 2061, 14, 243, 359, 1869, 2061, 16]\n",
      "with_eos first ids: [43, 359, 7348, 2061, 14, 243, 359, 1869, 2061, 16, 1]\n",
      "decode with_eos preview: I have 2 apples, you have 3 apples.<EOS>\n"
     ]
    }
   ],
   "source": [
    "cases = [\n",
    "    \"Once upon a time, a little cat was happy.\",\n",
    "    \"The sun is bright, and the kids can play outside!\",\n",
    "    \"I have 2 apples, you have 3 apples.\"\n",
    "]\n",
    "\n",
    "for text in cases:\n",
    "    ids_no_eos = tok.encode(text, add_eos=False)\n",
    "    ids_with_eos = tok.encode(text, add_eos=True)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEXT:\", text)\n",
    "    print(\"len no_eos:\", len(ids_no_eos), \"| len with_eos:\", len(ids_with_eos))\n",
    "    print(\"no_eos first ids:\", ids_no_eos[:20])\n",
    "    print(\"with_eos first ids:\", ids_with_eos[:20])\n",
    "    print(\"decode with_eos preview:\", tok.decode_ids(ids_with_eos)[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa776f6",
   "metadata": {},
   "source": [
    "## 10) Persist Artifacts and Reproducibility Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24473f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- /Users/m3/Documents/uni/s26/genai/lm_sampling_analysis/artifacts/tokenizer_state.json\n",
      "- /Users/m3/Documents/uni/s26/genai/lm_sampling_analysis/artifacts/tokenizer.json\n",
      "- /Users/m3/Documents/uni/s26/genai/lm_sampling_analysis/artifacts/tinygpt_baseline.pt\n",
      "- /Users/m3/Documents/uni/s26/genai/lm_sampling_analysis/artifacts/run_metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": [
    "tokenizer_state_path = ARTIFACTS_DIR / \"tokenizer_state.json\"\n",
    "model_ckpt_path = ARTIFACTS_DIR / \"tinygpt_baseline.pt\"\n",
    "run_meta_path = ARTIFACTS_DIR / \"run_metadata.json\"\n",
    "\n",
    "tokenizer_state = tok.to_state()\n",
    "with open(tokenizer_state_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_state, f)\n",
    "\n",
    "tok.save(ARTIFACTS_DIR / \"tokenizer.json\")\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            \"vocab_size\": VOCAB_SIZE_MODEL,\n",
    "            \"context_len\": CONTEXT_LEN - 1,\n",
    "            \"d_model\": 256,\n",
    "            \"n_heads\": 4,\n",
    "            \"n_layers\": 4,\n",
    "            \"ff_dim\": 1024,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    model_ckpt_path,\n",
    ")\n",
    "\n",
    "def safe_cmd(cmd):\n",
    "    try:\n",
    "        return subprocess.check_output(cmd, shell=True, text=True).strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "metadata = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"seed\": SEED,\n",
    "    \"run_mode\": RUN_MODE,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "    \"dataset\": \"roneneldan/TinyStories\",\n",
    "    \"stories_used\": len(train_texts),\n",
    "    \"context_len\": CONTEXT_LEN,\n",
    "    \"tokenizer_vocab_target\": TOKENIZER_VOCAB_SIZE,\n",
    "    \"tokenizer_vocab_actual\": len(tok.vocabulary),\n",
    "    \"special_tokens\": tok.special_tokens,\n",
    "    \"model_params\": int(sum(p.numel() for p in model.parameters())),\n",
    "    \"train_loss_initial\": float(train_losses[0]) if len(train_losses) else None,\n",
    "    \"train_loss_final\": float(train_losses[-1]) if len(train_losses) else None,\n",
    "    \"val_loss\": float(val_loss),\n",
    "    \"val_ppl\": float(val_ppl),\n",
    "    \"git_commit\": safe_cmd(\"git rev-parse --short HEAD\"),\n",
    "    \"workspace\": str(WORKSPACE),\n",
    "}\n",
    "\n",
    "with open(run_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", tokenizer_state_path)\n",
    "print(\"-\", ARTIFACTS_DIR / \"tokenizer.json\")\n",
    "print(\"-\", model_ckpt_path)\n",
    "print(\"-\", run_meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9bc6",
   "metadata": {},
   "source": [
    "## 11) Generate Sample Stories\n",
    "\n",
    "These cells generate a few readable stories from the trained model using prompt-based decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3c0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_story(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.9,\n",
    "    top_k=40,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    eos_id = tokenizer.special_tokens.get(\"<EOS>\")\n",
    "    input_ids = tokenizer.encode(prompt, add_eos=False)\n",
    "    if len(input_ids) == 0:\n",
    "        input_ids = [eos_id] if eos_id is not None else [0]\n",
    "\n",
    "    x = torch.tensor([input_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -model.context_len:]\n",
    "        logits = model(x_cond)\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        if temperature <= 0:\n",
    "            next_token = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            next_logits = next_logits / temperature\n",
    "            if top_k is not None and top_k > 0:\n",
    "                top_k = min(top_k, next_logits.shape[-1])\n",
    "                values, _ = torch.topk(next_logits, top_k)\n",
    "                cutoff = values[:, -1].unsqueeze(-1)\n",
    "                next_logits = torch.where(\n",
    "                    next_logits < cutoff,\n",
    "                    torch.full_like(next_logits, torch.finfo(next_logits.dtype).min),\n",
    "                    next_logits,\n",
    "                )\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        if eos_id is not None and next_token.item() == eos_id:\n",
    "            break\n",
    "\n",
    "    output_ids = x[0].tolist()\n",
    "\n",
    "    if eos_id is not None and eos_id in output_ids:\n",
    "        eos_pos = output_ids.index(eos_id)\n",
    "        output_ids = output_ids[:eos_pos]\n",
    "\n",
    "    return tokenizer.decode_ids(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe38227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Story 1 | Prompt: 'Once upon a time'\n",
      "------------------------------------------------------------------------------------------\n",
      "Once upon a time, there was a little boy who wanted to help. Every day they would play the park for a walk.\n",
      "\n",
      "One day, he saw a big pile of animals, white, blue, the clouds. He wanted to play with it, so he asked his friend if he could help his friends to play with.\n",
      "\n",
      "When his friend, the man said, \"The man, can I have some blocks. We are very creative.\" Sammy smiled and said, \"Yes, please! It's very fun!\" They played together every day and became fun.\n",
      "\n",
      "==========================================================================================\n",
      "Story 2 | Prompt: 'The little cat'\n",
      "------------------------------------------------------------------------------------------\n",
      "The little cat said, \"I'm afraid of fun!\"\n",
      "\n",
      "The two friends were happy and they both laughed with their work together. They all were happy because they had a great day and enjoyed spending time, but it's time to go on the slide. \n",
      "\n",
      "But one day, a big bird named Billy came to find the rabbit. The dog was very angry and said, \"You're welcome, little dog.\" The little dog looked so happy and said, \"That's great, it's okay to always visit my stick for you.\" He went home, and the cat were happy again. He\n",
      "\n",
      "==========================================================================================\n",
      "Story 3 | Prompt: 'In a small red house'\n",
      "------------------------------------------------------------------------------------------\n",
      "In a small red house. They laugh and laughed.\n",
      "\n",
      "The sun was a little happy dog, so he could be safe soon. He ran to his home with his tail.\n",
      "\n",
      "Tommy was very happy. He looked around in the big room and it became good friends. They had so much fun that he was very strong and had a great time.\n",
      "\n",
      "==========================================================================================\n",
      "Story 4 | Prompt: 'Tom and Anna went to the park'\n",
      "------------------------------------------------------------------------------------------\n",
      "Tom and Anna went to the park. The water was so happy to have a fun place. And she saw a little girl named Lily. Her parents felt so grateful for the little girl. She promised to be the girl she had to come and play with the animals.\n",
      "\n",
      "==========================================================================================\n",
      "Story 5 | Prompt: 'The sun was warm and'\n",
      "------------------------------------------------------------------------------------------\n",
      "The sun was warm and the little girl was safe and had to go on something special to be perfect for him.\n"
     ]
    }
   ],
   "source": [
    "sample_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little cat\",\n",
    "    \"In a small red house\",\n",
    "    \"Tom and Anna went to the park\",\n",
    "    \"The sun was warm and\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(sample_prompts, start=1):\n",
    "    story = generate_story(\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=120,\n",
    "        temperature=0.9,\n",
    "        top_k=40,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Story {i} | Prompt: {prompt!r}\")\n",
    "    print('-'*90)\n",
    "    print(story)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
